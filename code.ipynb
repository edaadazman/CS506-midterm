{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1697533, 9)\n",
      "Test shape: (212192, 2)\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For text processing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# For model building and evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Machine Learning models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier  # Add this line\n",
    "\n",
    "# To handle sparse matrices\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the data\n",
    "train_df = pd.read_csv('./data/train.csv')\n",
    "test_df = pd.read_csv('./data/test.csv')\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in training data:\n",
      "Id                             0\n",
      "ProductId                      0\n",
      "UserId                         0\n",
      "HelpfulnessNumerator           0\n",
      "HelpfulnessDenominator         0\n",
      "Time                           0\n",
      "Summary                       32\n",
      "Text                          62\n",
      "Score                     212192\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Display first few rows of the training data\n",
    "train_df.head()\n",
    "\n",
    "# Check for missing values in training data\n",
    "print(\"Missing values in training data:\")\n",
    "print(train_df.isnull().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All test Ids are present in test_data.\n"
     ]
    }
   ],
   "source": [
    "def add_features_to(df):\n",
    "    # Handle missing values in 'HelpfulnessDenominator' to avoid division by zero\n",
    "    df['HelpfulnessDenominator'] = df['HelpfulnessDenominator'].replace(0, np.nan)\n",
    "    df['Helpfulness'] = df['HelpfulnessNumerator'] / df['HelpfulnessDenominator']\n",
    "    df['Helpfulness'] = df['Helpfulness'].fillna(0)\n",
    "\n",
    "    # Convert 'Time' to datetime and extract components\n",
    "    df['Time'] = pd.to_datetime(df['Time'], unit='s')\n",
    "    df['Review_Year'] = df['Time'].dt.year\n",
    "    df['Review_Month'] = df['Time'].dt.month\n",
    "    df['Review_Day'] = df['Time'].dt.day\n",
    "\n",
    "    # Fill NaN in 'Summary' and 'Text'\n",
    "    df['Summary'] = df['Summary'].fillna('')\n",
    "    df['Text'] = df['Text'].fillna('')\n",
    "\n",
    "    # Length of 'Summary' and 'Text'\n",
    "    df['Summary_length'] = df['Summary'].apply(len)\n",
    "    df['Text_length'] = df['Text'].apply(len)\n",
    "\n",
    "    # Word count in 'Summary' and 'Text'\n",
    "    df['Summary_word_count'] = df['Summary'].apply(lambda x: len(x.split()))\n",
    "    df['Text_word_count'] = df['Text'].apply(lambda x: len(x.split()))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the function to the entire train_df\n",
    "train_df = add_features_to(train_df)\n",
    "\n",
    "# Training data: Rows where 'Score' is not null\n",
    "train_data = train_df[train_df['Score'].notnull()]\n",
    "\n",
    "# Test data: Rows where 'Id' is in test_df['Id'] and 'Score' is null\n",
    "test_data = train_df[train_df['Id'].isin(test_df['Id']) & train_df['Score'].isnull()]\n",
    "\n",
    "# Verify that all test IDs are included\n",
    "missing_ids = set(test_df['Id']) - set(test_data['Id'])\n",
    "if len(missing_ids) > 0:\n",
    "    print(f\"Missing Ids in test_data: {missing_ids}\")\n",
    "else:\n",
    "    print(\"All test Ids are present in test_data.\")\n",
    "\n",
    "# If any Ids are missing, you may need to adjust the approach.\n",
    "\n",
    "# Merge test_df with train_df to get test_data\n",
    "test_data = pd.merge(test_df[['Id']], train_df.drop(columns=['Score']), on='Id', how='left')\n",
    "\n",
    "# Combine 'Summary' and 'Text' in train and test data\n",
    "train_data['Combined_Text'] = train_data['Summary'] + ' ' + train_data['Text']\n",
    "test_data['Combined_Text'] = test_data['Summary'] + ' ' + test_data['Text']\n",
    "\n",
    "# Combine the data for vectorization\n",
    "combined_data = pd.concat([train_data['Combined_Text'], test_data['Combined_Text']], axis=0)\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,2))\n",
    "\n",
    "# Fit and transform the combined text data\n",
    "tfidf_combined_text = tfidf_vectorizer.fit_transform(combined_data)\n",
    "\n",
    "# Split the vectorized data back into train and test sets\n",
    "tfidf_train = tfidf_combined_text[:len(train_data)]\n",
    "tfidf_test = tfidf_combined_text[len(train_data):]\n",
    "\n",
    "# List of numerical features\n",
    "numeric_features = ['HelpfulnessNumerator', 'HelpfulnessDenominator', 'Helpfulness',\n",
    "                    'Summary_length', 'Text_length', 'Summary_word_count', 'Text_word_count',\n",
    "                    'Review_Year', 'Review_Month', 'Review_Day']\n",
    "\n",
    "# Prepare numeric features for training and testing data\n",
    "X_train_numeric = train_data[numeric_features].fillna(0)\n",
    "X_test_numeric = test_data[numeric_features].fillna(0)\n",
    "\n",
    "# Combine TF-IDF features with numeric features\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "X_train_combined = hstack([tfidf_train, csr_matrix(X_train_numeric.values)])\n",
    "X_test_combined = hstack([tfidf_test, csr_matrix(X_test_numeric.values)])\n",
    "\n",
    "# Define the target variable\n",
    "y_train = train_data['Score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building and Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edaad/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.5480642867914772\n",
      "Training Multinomial Naive Bayes model...\n",
      "Multinomial Naive Bayes Accuracy: 0.45190878341986773\n",
      "Training Random Forest Classifier model...\n",
      "Random Forest Classifier Accuracy: 0.39996660706206777\n"
     ]
    }
   ],
   "source": [
    "# Split the training data into training and validation sets\n",
    "X_train_part, X_valid, y_train_part, y_valid = train_test_split(\n",
    "    X_train_combined, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "print(\"Training Logistic Regression model...\")\n",
    "logreg = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
    "logreg.fit(X_train_part, y_train_part)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_logreg = logreg.predict(X_valid)\n",
    "accuracy_logreg = accuracy_score(y_valid, y_pred_logreg)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_logreg)\n",
    "\n",
    "print(\"Training Multinomial Naive Bayes model...\")\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_part, y_train_part)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_mnb = mnb.predict(X_valid)\n",
    "accuracy_mnb = accuracy_score(y_valid, y_pred_mnb)\n",
    "print(\"Multinomial Naive Bayes Accuracy:\", accuracy_mnb)\n",
    "\n",
    "# Optional: Dimensionality reduction before Random Forest\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=100)\n",
    "X_train_reduced = svd.fit_transform(X_train_combined)\n",
    "X_valid_reduced = svd.transform(X_valid)\n",
    "\n",
    "print(\"Training Random Forest Classifier model...\")\n",
    "rf_classifier = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "rf_classifier.fit(X_train_reduced[:len(y_train_part)], y_train_part)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_rf = rf_classifier.predict(X_valid_reduced)\n",
    "accuracy_rf = accuracy_score(y_valid, y_pred_rf)\n",
    "print(\"Random Forest Classifier Accuracy:\", accuracy_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracies:\n",
      "Logistic Regression: 0.5480642867914772\n",
      "Multinomial Naive Bayes: 0.45190878341986773\n",
      "Random Forest Classifier: 0.39996660706206777\n"
     ]
    }
   ],
   "source": [
    "# Compare accuracies\n",
    "print(\"Model Accuracies:\")\n",
    "print(f\"Logistic Regression: {accuracy_logreg}\")\n",
    "print(f\"Multinomial Naive Bayes: {accuracy_mnb}\")\n",
    "print(f\"Random Forest Classifier: {accuracy_rf}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model Training and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created successfully!\n"
     ]
    }
   ],
   "source": [
    "# If Logistic Regression is the best model\n",
    "y_test_pred = logreg.predict(X_test_combined)\n",
    "\n",
    "# Prepare submission file\n",
    "submission = test_df[['Id']].copy()\n",
    "submission['Score'] = y_test_pred\n",
    "\n",
    "# Save to CSV\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Submission file created successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
